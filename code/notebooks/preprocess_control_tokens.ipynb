{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) Facebook, Inc. and its affiliates.\n",
    "# All rights reserved.\n",
    "#\n",
    "# This source code is licensed under the license found in the\n",
    "# LICENSE file in the root directory of this source https://github.com/facebookresearch/access/tree/main/access\n",
    "#\n",
    "\n",
    "from functools import lru_cache\n",
    "import re\n",
    "from string import punctuation\n",
    "\n",
    "from nltk.tokenize.nist import NISTTokenizer\n",
    "from nltk.corpus import stopwords as nltk_stopwords\n",
    "import spacy\n",
    "\n",
    "# TODO: #language_specific\n",
    "stopwords = set(nltk_stopwords.words('english'))\n",
    "\n",
    "\n",
    "@lru_cache(maxsize=100)  # To speed up subsequent calls\n",
    "def word_tokenize(sentence):\n",
    "    tokenizer = NISTTokenizer()\n",
    "    sentence = ' '.join(tokenizer.tokenize(sentence))\n",
    "    # Rejoin special tokens that where tokenized by error: e.g. \"<PERSON_1>\" -> \"< PERSON _ 1 >\"\n",
    "    for match in re.finditer(r'< (?:[A-Z]+ _ )+\\d+ >', sentence):\n",
    "        sentence = sentence.replace(match.group(), ''.join(match.group().split()))\n",
    "    return sentence\n",
    "\n",
    "\n",
    "def to_words(sentence):\n",
    "    return sentence.split()\n",
    "\n",
    "\n",
    "def remove_punctuation_characters(text):\n",
    "    return ''.join([char for char in text if char not in punctuation])\n",
    "\n",
    "\n",
    "@lru_cache(maxsize=1000)\n",
    "def is_punctuation(word):\n",
    "    return remove_punctuation_characters(word) == ''\n",
    "\n",
    "\n",
    "@lru_cache(maxsize=100)\n",
    "def remove_punctuation_tokens(text):\n",
    "    return ' '.join([w for w in to_words(text) if not is_punctuation(w)])\n",
    "\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    return ' '.join([w for w in to_words(text) if w.lower() not in stopwords])\n",
    "\n",
    "\n",
    "@lru_cache(maxsize=1)\n",
    "def get_spacy_model():\n",
    "    model = 'ja_core_news_sm'\n",
    "    if not spacy.util.is_package(model):\n",
    "        spacy.cli.download(model)\n",
    "        spacy.cli.link(model, model, force=True, model_path=spacy.util.get_package_path(model))\n",
    "    return spacy.load(model)  # python -m spacy download en_core_web_sm`\n",
    "\n",
    "\n",
    "@lru_cache(maxsize=10**6)\n",
    "def spacy_process(text):\n",
    "    return get_spacy_model()(str(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<NC_2.0> \n",
      "0.75\n",
      "1.0\n",
      "1.0\n",
      "<NC_0.55>  <LS_0.7>  <DR_1.0>  <WR_1.0> \n",
      "<NC_0.65>  <LS_0.8>  <DR_1.0>  <WR_1.45> \n",
      "<NC_0.35>  <LS_0.5>  <DR_1.0>  <WR_1.15> \n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import Levenshtein\n",
    "from pathlib import Path\n",
    "import fasttext\n",
    "\n",
    "\n",
    "FASTTEXT_EMBEDDINGS_PATH = \"/Users/michaelryan/Documents/School/GeorgiaTech/Research/MultilingualSimplification/fastText/cc.ja.300.bin\"\n",
    "\n",
    "\n",
    "def get_char_ratio(complex, simple):\n",
    "\treturn len(simple) / len(complex)\n",
    "\n",
    "\n",
    "def get_levenshtein_similarity(complex_sentence, simple_sentence):\n",
    "\treturn Levenshtein.ratio(complex_sentence, simple_sentence)\n",
    "\n",
    "\n",
    "def get_bucket(ratio, feat=\"NC\"):\n",
    "\tbucket = round(min(math.ceil(ratio / 0.05), 40) * 0.05, 2)\n",
    "\treturn \"<\" + feat + \"_\" + str(bucket) + \"> \"\n",
    "\n",
    "\n",
    "def get_depth_ratio(complex, simple):\n",
    "\tdenom = get_dependency_tree_depth(complex)\n",
    "\tif denom == 0:\n",
    "\t\treturn 0\n",
    "\treturn get_dependency_tree_depth(simple) * 1.0 / denom\n",
    "\n",
    "\n",
    "def get_dependency_tree_depth(sentence):\n",
    "\tdef get_subtree_depth(node):\n",
    "\t\tif len(list(node.children)) == 0:\n",
    "\t\t\treturn 0\n",
    "\t\treturn 1 + max([get_subtree_depth(child) for child in node.children])\n",
    "\n",
    "\ttree_depths = [get_subtree_depth(spacy_sentence.root) for spacy_sentence in spacy_process(sentence).sents]\n",
    "\tif len(tree_depths) == 0:\n",
    "\t\treturn 0\n",
    "\treturn max(tree_depths)\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from functools import lru_cache\n",
    "\n",
    "\n",
    "def yield_lines(filepath, n_lines=float('inf'), prop=1):\n",
    "\tif prop < 1:\n",
    "\t\tassert n_lines == float('inf')\n",
    "\t\tn_lines = int(prop * count_lines(filepath))\n",
    "\twith open(filepath, 'r') as f:\n",
    "\t\tfor i, l in enumerate(f):\n",
    "\t\t\tif i >= n_lines:\n",
    "\t\t\t\tbreak\n",
    "\t\t\tyield l.rstrip('\\n')\n",
    "\n",
    "\n",
    "def count_lines(filepath):\n",
    "\tn_lines = 0\n",
    "\twith Path(filepath).open() as f:\n",
    "\t\tfor l in f:\n",
    "\t\t\tn_lines += 1\n",
    "\treturn n_lines\n",
    "\n",
    "\n",
    "@lru_cache(maxsize=1)\n",
    "def get_word2rank(vocab_size=np.inf):\n",
    "\t# TODO: Decrease vocab size or load from smaller file\n",
    "\tword2rank = {}\n",
    "\tmodel = fasttext.load_model(FASTTEXT_EMBEDDINGS_PATH)\n",
    "\tfor i, word in enumerate(model.get_words(on_unicode_error='replace')):\n",
    "\t\tif (i + 1) > vocab_size:\n",
    "\t\t\tbreak\n",
    "\t\tword2rank[word] = i\n",
    "\treturn word2rank\n",
    "\n",
    "\n",
    "def get_rank(word):\n",
    "\treturn get_word2rank().get(word, len(get_word2rank()))\n",
    "\n",
    "\n",
    "def get_log_rank(word):\n",
    "\treturn np.log(1 + get_rank(word))\n",
    "\n",
    "\n",
    "def get_lexical_complexity_score(sentence):\n",
    "\twords = to_words(remove_stopwords(remove_punctuation_tokens(sentence)))\n",
    "\twords = [word for word in words if word in get_word2rank()]\n",
    "\tif len(words) == 0:\n",
    "\t\treturn np.log(1 + len(get_word2rank()))  # TODO: This is completely arbitrary\n",
    "\treturn np.quantile([get_log_rank(word) for word in words], 0.75)\n",
    "\n",
    "\n",
    "def get_word_rank(complex, simple):\n",
    "\tdenom = get_lexical_complexity_score(complex)\n",
    "\tif denom == 0:\n",
    "\t\treturn 0\n",
    "\treturn get_lexical_complexity_score(simple) * 1.0 / denom\n",
    "\n",
    "\n",
    "def get_prefix(complex, simple):\n",
    "\tprefixes = []\n",
    "\tprefixes.append(get_bucket(get_char_ratio(complex, simple), \"NC\"))\n",
    "\tprefixes.append(get_bucket(get_levenshtein_similarity(complex, simple), \"LS\"))\n",
    "\tprefixes.append(get_bucket(get_depth_ratio(complex, simple), \"DR\"))\n",
    "\tprefixes.append(get_bucket(get_word_rank(complex, simple), \"WR\"))\n",
    "\treturn \" \".join(prefixes)\n",
    "\n",
    "def get_prefix_precomputed(chr_ratio, lev_sim, dep_ratio, word_rank):\n",
    "\tprefixes = []\n",
    "\tprefixes.append(get_bucket(chr_ratio, \"NC\"))\n",
    "\tprefixes.append(get_bucket(lev_sim, \"LS\"))\n",
    "\tprefixes.append(get_bucket(dep_ratio, \"DR\"))\n",
    "\tprefixes.append(get_bucket(word_rank, \"WR\"))\n",
    "\treturn \" \".join(prefixes)\n",
    "\n",
    "print(get_bucket(2.81))\n",
    "print(get_levenshtein_similarity(\"asdf\", \"asbf\"))\n",
    "print(get_depth_ratio(\"This is a test.\", \"This is.\"))\n",
    "print(get_word_rank(\"This is a test.\", \"This is.\"))\n",
    "print(get_prefix(\"This is a test.\", \"This is.\"))\n",
    "print(get_prefix(\"C'est un test.\",\"C'est un.\"))\n",
    "print(get_prefix(\"Это проверка.\", \"Это.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_control_tokens_to_df(df, override_dr=None):\n",
    "    original = []\n",
    "    simple = []\n",
    "\n",
    "    char_ratios = []\n",
    "    lev_sims = []\n",
    "    depth_ratios = []\n",
    "    word_ranks = []\n",
    "\n",
    "    with tqdm(total=len(df)) as progress_bar:\n",
    "        for i, content in tqdm(df.iterrows()):\n",
    "            chr_ratio = get_char_ratio(content['original'], content['simple'])\n",
    "            lev_sim = get_levenshtein_similarity(content['original'], content['simple'])\n",
    "            dep_ratio = override_dr\n",
    "            if not override_dr:\n",
    "                dep_ratio = get_depth_ratio(content['original'], content['simple'])\n",
    "                \n",
    "            word_rank = get_word_rank(content['original'], content['simple'])\n",
    "\n",
    "            prefix = get_prefix_precomputed(chr_ratio, lev_sim, dep_ratio, word_rank)\n",
    "\n",
    "            char_ratios.append(chr_ratio)\n",
    "            lev_sims.append(lev_sim)\n",
    "            depth_ratios.append(dep_ratio)\n",
    "            word_ranks.append(word_rank)\n",
    "\n",
    "            original.append(prefix + content['original'])\n",
    "            simple.append(content['simple'])\n",
    "            progress_bar.update(1)\n",
    "\n",
    "    char_ratios = np.array(char_ratios)\n",
    "    lev_sims = np.array(lev_sims)\n",
    "    depth_ratios = np.array(depth_ratios)\n",
    "    word_ranks = np.array(word_ranks)\n",
    "\n",
    "    avg_prefix = get_prefix_precomputed(\n",
    "        np.average(char_ratios),\\\n",
    "        np.average(lev_sims),\\\n",
    "        np.average(depth_ratios),\\\n",
    "        np.average(word_ranks))\n",
    "    \n",
    "    return pd.DataFrame({\"original\": original, \"simple\": simple}), avg_prefix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "27600it [02:19, 198.04it/s]0 [02:19<00:00, 202.72it/s]\n",
      "100%|██████████| 27600/27600 [02:19<00:00, 198.04it/s]\n"
     ]
    }
   ],
   "source": [
    "df, prefix = add_control_tokens_to_df(pd.read_csv(\"../data/Japanese/Easy Japanese Corpus_train.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>original</th>\n",
       "      <th>simple</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;NC_1.0&gt;  &lt;LS_0.9&gt;  &lt;DR_1.0&gt;  &lt;WR_1.0&gt; 彼は私を見て危...</td>\n",
       "      <td>彼は私を見て危険だといいました。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;NC_0.95&gt;  &lt;LS_0.8&gt;  &lt;DR_1.0&gt;  &lt;WR_1.0&gt; もう授業中に...</td>\n",
       "      <td>もう授業中には決して勝手に話しません。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>&lt;NC_1.1&gt;  &lt;LS_0.75&gt;  &lt;DR_1.5&gt;  &lt;WR_1.0&gt; 午前中は在宅...</td>\n",
       "      <td>午前中は家にいる予定です。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>&lt;NC_1.1&gt;  &lt;LS_0.95&gt;  &lt;DR_1.0&gt;  &lt;WR_1.0&gt; 電話を切らず...</td>\n",
       "      <td>電話を切らずにちょっとお待ちください。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>&lt;NC_1.0&gt;  &lt;LS_0.95&gt;  &lt;DR_1.0&gt;  &lt;WR_1.0&gt; 彼の妻はもち...</td>\n",
       "      <td>彼の妻はもちろん子供たちもそのパーティーに呼ばれた。</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            original  \\\n",
       "0  <NC_1.0>  <LS_0.9>  <DR_1.0>  <WR_1.0> 彼は私を見て危...   \n",
       "1  <NC_0.95>  <LS_0.8>  <DR_1.0>  <WR_1.0> もう授業中に...   \n",
       "2  <NC_1.1>  <LS_0.75>  <DR_1.5>  <WR_1.0> 午前中は在宅...   \n",
       "3  <NC_1.1>  <LS_0.95>  <DR_1.0>  <WR_1.0> 電話を切らず...   \n",
       "4  <NC_1.0>  <LS_0.95>  <DR_1.0>  <WR_1.0> 彼の妻はもち...   \n",
       "\n",
       "                       simple  \n",
       "0            彼は私を見て危険だといいました。  \n",
       "1         もう授業中には決して勝手に話しません。  \n",
       "2               午前中は家にいる予定です。  \n",
       "3         電話を切らずにちょっとお待ちください。  \n",
       "4  彼の妻はもちろん子供たちもそのパーティーに呼ばれた。  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<NC_1.1>  <LS_0.8>  <DR_1.15>  <WR_1.0> \n"
     ]
    }
   ],
   "source": [
    "print(prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_ctrl_tokens(path_to_modify, store_averages=\"../misc/average_train_prefix.txt\", override_dr = None):\n",
    "    df = pd.read_csv(path_to_modify)\n",
    "    df, prefix = add_control_tokens_to_df(df, override_dr)\n",
    "    with open(store_averages, 'a') as f:\n",
    "        f.write(os.path.basename(path_to_modify) + \": \" + prefix + \"\\n\")\n",
    "    df.to_csv(path_to_modify, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "32248it [02:33, 210.58it/s]8 [02:33<00:00, 467.93it/s]\n",
      "100%|██████████| 32248/32248 [02:33<00:00, 210.58it/s]\n"
     ]
    }
   ],
   "source": [
    "add_ctrl_tokens(\"../data/Japanese/Easy Japanese Extended_train.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlts",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ca18bd5681216905704c89b7035c68c533f197e11f52b5f3035c7182103b2886"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
