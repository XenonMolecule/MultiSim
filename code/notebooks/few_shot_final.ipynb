{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import subprocess\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from numpy.random import default_rng\n",
    "from easse.sari import corpus_sari\n",
    "from sacrebleu import corpus_bleu\n",
    "import json\n",
    "import requests\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "token = os.environ.get(\"HUGGING_FACE_API_TOKEN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: LASER=../../../LASER\n"
     ]
    }
   ],
   "source": [
    "%env LASER=../../../LASER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def laser_embed(df, name, split, laser_version=\"\"):\n",
    "    txt_path = \"./laser_embeddings/\" + name + \"_\" + split + \".txt\"\n",
    "    bin_path = \"./laser_embeddings/\" + name + \"_\" + split + \".bin\"\n",
    "    with open(txt_path, 'w') as f:\n",
    "        for txt in df['original']:\n",
    "            f.write(txt.replace('\\n','') + '\\n')\n",
    "    subprocess.run([\"bash\",\"../../../LASER/tasks/embed/embed.sh\",txt_path,bin_path,laser_version])\n",
    "    os.remove(txt_path)\n",
    "\n",
    "def load_laser_embeddings(name, split):\n",
    "    dim = 1024\n",
    "    bin_path = \"./laser_embeddings/\" + name + \"_\" + split + \".bin\"\n",
    "\n",
    "    embeddings = np.fromfile(bin_path, dtype=np.float32, count=-1)                                                                          \n",
    "    embeddings.resize(embeddings.shape[0] // dim, dim)\n",
    "\n",
    "    return embeddings\n",
    "\n",
    "def calc_distances_to_neighbors(train_emb, eval_emb, neighbors):\n",
    "    # Find distances to all neighbors\n",
    "    A = train_emb[neighbors, :]\n",
    "    B = eval_emb\n",
    "\n",
    "    dot_product = np.dot(A, B.T).diagonal(0,0,2).T\n",
    "\n",
    "    # Compute the L2 norm of the vectors in A and B\n",
    "    norm_A = np.linalg.norm(A, axis=2)\n",
    "    norm_B = np.linalg.norm(B, axis=1)\n",
    "\n",
    "    # Compute the cosine distance between each pair of vectors using broadcasting\n",
    "    cosine_distances = 1 - (dot_product / (norm_A.T * norm_B).T)\n",
    "\n",
    "    return cosine_distances\n",
    "\n",
    "def generate_preprocessing_sim(name, train_emb, eval_emb, split=\"test\"):\n",
    "    K=20\n",
    "\n",
    "    model = NearestNeighbors(n_neighbors=K,\n",
    "                            metric='cosine',\n",
    "                            algorithm='brute',\n",
    "                            n_jobs=-1)\n",
    "    model.fit(train_emb)\n",
    "\n",
    "    closest_neighbors = model.kneighbors(eval_emb, return_distance=False)\n",
    "\n",
    "    cosine_distances = calc_distances_to_neighbors(train_emb, eval_emb, closest_neighbors)\n",
    "\n",
    "    pd.DataFrame(closest_neighbors).to_csv(\"./few_shot_preprocessing/\" + name + \"_\" + split + \"_similarity.csv\")\n",
    "    pd.DataFrame(cosine_distances).to_csv(\"./few_shot_preprocessing/\" + name + \"_\" + split + \"_similarity_dist.csv\")\n",
    "\n",
    "def generate_preprocessing_rand(name, train_emb, eval_emb, split=\"test\"):\n",
    "    K = 20\n",
    "\n",
    "    rng = np.random.default_rng(3600)\n",
    "    random_neighbors = rng.integers(low=0, high=train_emb.shape[0], size=(eval_emb.shape[0], K))\n",
    "    cosine_distances = calc_distances_to_neighbors(train_emb, eval_emb, random_neighbors)\n",
    "\n",
    "    pd.DataFrame(random_neighbors).to_csv(\"./few_shot_preprocessing/\" + name + \"_\" + split + \"_random.csv\")\n",
    "    pd.DataFrame(cosine_distances).to_csv(\"./few_shot_preprocessing/\" + name + \"_\" + split + \"_random_dist.csv\")\n",
    "\n",
    "\n",
    "def preprocess_dataset(train_path, test_path, name, split=\"test\"):\n",
    "    train = pd.read_csv(train_path)\n",
    "    test = pd.read_csv(test_path)\n",
    "\n",
    "    laser_version = \"\"\n",
    "    if name == \"SimplifyUR\":\n",
    "        laser_version = \"urd_Arab\"\n",
    "    \n",
    "    laser_embed(train, name, \"train\", laser_version)\n",
    "    laser_embed(test, name, \"test\", laser_version)\n",
    "\n",
    "    train_embeddings = load_laser_embeddings(name, \"train\")\n",
    "    test_embeddings = load_laser_embeddings(name, \"test\")\n",
    "\n",
    "    generate_preprocessing_sim(name, train_embeddings, test_embeddings, split)\n",
    "    generate_preprocessing_rand(name, train_embeddings, test_embeddings, split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-12-29 12:24:05,866 | INFO | embed | spm_model: /Users/michaelryan/Documents/School/GeorgiaTech/Research/LASER/nllb/laser2.spm\n",
      "2022-12-29 12:24:05,866 | INFO | embed | spm_cvocab: /Users/michaelryan/Documents/School/GeorgiaTech/Research/LASER/nllb/laser2.cvocab\n",
      "2022-12-29 12:24:05,866 | INFO | embed | loading encoder: /Users/michaelryan/Documents/School/GeorgiaTech/Research/LASER/nllb/laser2.pt\n",
      "2022-12-29 12:24:06,198 | INFO | preprocess | SPM processing TSSlovene_train.txt  \n",
      "2022-12-29 12:24:06,305 | INFO | embed | encoding /var/folders/zy/zlsw34jx4zn2cv4tn33_02nh0000gn/T/tmp44dy4sj6/spm to ./laser_embeddings/TSSlovene_train.bin\n",
      "2022-12-29 12:24:11,244 | INFO | embed | encoded 749 sentences in 4s\n",
      "2022-12-29 12:24:12,347 | INFO | embed | spm_model: /Users/michaelryan/Documents/School/GeorgiaTech/Research/LASER/nllb/laser2.spm\n",
      "2022-12-29 12:24:12,347 | INFO | embed | spm_cvocab: /Users/michaelryan/Documents/School/GeorgiaTech/Research/LASER/nllb/laser2.cvocab\n",
      "2022-12-29 12:24:12,347 | INFO | embed | loading encoder: /Users/michaelryan/Documents/School/GeorgiaTech/Research/LASER/nllb/laser2.pt\n",
      "2022-12-29 12:24:12,619 | INFO | preprocess | SPM processing TSSlovene_test.txt  \n",
      "2022-12-29 12:24:12,671 | INFO | embed | encoding /var/folders/zy/zlsw34jx4zn2cv4tn33_02nh0000gn/T/tmpj5h0jv6l/spm to ./laser_embeddings/TSSlovene_test.bin\n",
      "2022-12-29 12:24:13,581 | INFO | embed | encoded 96 sentences in 0s\n"
     ]
    }
   ],
   "source": [
    "preprocess_dataset(\"../data/Slovene/Text Simplification Slovene_train.csv\", \"../data/Slovene/Text Simplification Slovene_test.csv\", \"TSSlovene\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_bleu_sari(df_ref, sentences):\n",
    "\n",
    "    num_refs = df_ref.shape[1]-1\n",
    "\n",
    "    bleu_scores = np.zeros((num_refs))\n",
    "    sari_scores = np.zeros((num_refs))\n",
    "\n",
    "    examples = [{\"original\": [], \"sentences\": [], \"references\": []} for _ in range(num_refs)]\n",
    "\n",
    "    assert df_ref.shape[0] == len(sentences)\n",
    "\n",
    "    for (index,row), sentence in zip(df_ref.iterrows(), sentences):\n",
    "        original = row['original']\n",
    "        simple = sentence\n",
    "        ref_list = []\n",
    "        for col in row.index:\n",
    "            if col != 'original' and type(row[col]) != float:\n",
    "                ref_list.append(row[col])\n",
    "        num_ref = len(ref_list)\n",
    "        examples[num_ref-1]['original'].append(original)\n",
    "        examples[num_ref-1]['sentences'].append(simple)\n",
    "        examples[num_ref-1]['references'].append(ref_list)\n",
    "\n",
    "    counts = np.array([len(e['original']) for e in examples])\n",
    "    total = sum(counts)\n",
    "    weights = np.divide(counts, total)\n",
    "\n",
    "    for i in range(len(examples)):\n",
    "        if counts[i] > 0:\n",
    "            references = np.array(examples[i]['references']).T.tolist()\n",
    "            bleu_scores[i] = corpus_bleu(\n",
    "                                examples[i]['sentences'],\n",
    "                                references,\n",
    "                                force = True,\n",
    "                                tokenize = '13a',\n",
    "                                lowercase = True\n",
    "                            ).score\n",
    "            sari_scores[i] = corpus_sari(\n",
    "                                orig_sents = examples[i]['original'],\n",
    "                                sys_sents = examples[i]['sentences'],\n",
    "                                refs_sents = references,\n",
    "                                tokenizer = '13a'\n",
    "                            )\n",
    "    \n",
    "    bleu = np.dot(bleu_scores, weights)\n",
    "    sari = np.dot(sari_scores, weights)\n",
    "\n",
    "    return bleu, sari"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "API_URL = \"https://api-inference.huggingface.co/models/bigscience/bloom\"\n",
    "headers = {\"Authorization\": f\"Bearer {token}\"}\n",
    "def query(payload):\n",
    "    data = json.dumps(payload)\n",
    "    response = requests.request(\"POST\", API_URL, headers=headers, data=data)\n",
    "    return json.loads(response.content.decode(\"utf-8\"))\n",
    "\n",
    "def load_fewshot_examples(train, test, mapping, offset=0):\n",
    "    output = defaultdict(lambda:[])\n",
    "    for j, (example,ref) in enumerate(zip(test['original'], test['simple'])):\n",
    "        output['original'].append(example)\n",
    "        output['ref'].append(ref)\n",
    "        i_off = 0\n",
    "        for i, idx in enumerate(mapping.iloc[j]):\n",
    "            if i != 0 and i > offset:\n",
    "                output[\"ex\" + str(i_off) + \"_orig\"].append(train.iloc[idx][\"original\"])\n",
    "                output[\"ex\" + str(i_off) + \"_simp\"].append(train.iloc[idx][\"simple\"])\n",
    "                i_off += 1\n",
    "    \n",
    "    out_df = pd.DataFrame(output)\n",
    "    return out_df\n",
    "    \n",
    "def construct_example(example_row, k=3):\n",
    "    output = []\n",
    "    for i in range(k):\n",
    "        output.append(\"Original: \\\"\" + example_row[\"ex\" + str(i) +\"_orig\"] + \"\\\"\\n\")\n",
    "        output.append(\"Simple: \\\"\" + example_row[\"ex\" + str(i) + \"_simp\"] + \"\\\"\\n\\n\")\n",
    "\n",
    "    output.append(\"Original: \\\"\" + example_row[\"original\"] + \"\\\"\\nSimple: \\\"\")\n",
    "    return \"\".join(output)\n",
    "\n",
    "REQUERY_LIMIT = 5\n",
    "def generate_fewshot(example_row, k=3):\n",
    "    ex = construct_example(example_row, k=k)\n",
    "\n",
    "    new = \"\"\n",
    "    new_total = \"\"\n",
    "    for i in range(REQUERY_LIMIT):\n",
    "        response = query(ex)\n",
    "        res = response[0]['generated_text']\n",
    "        new = res[len(ex):]\n",
    "        new_total += res[len(ex):]\n",
    "        if \"\\\"\\n\"\"\" in new_total:\n",
    "            return new_total.split(\"\\\"\\n\"\"\")[0]\n",
    "        elif \"Original:\" in new_total:\n",
    "            return new_total.split(\"Original:\")[0]\n",
    "        else:\n",
    "            ex += new\n",
    "    return new_total\n",
    "\n",
    "def fewshot_eval(train_path, test_path, preprocessed_path, k=3, output_csv=\"\", checkpoint=\"\"):\n",
    "    train = pd.read_csv(train_path)\n",
    "    test = pd.read_csv(test_path)\n",
    "    preprocessed = pd.read_csv(preprocessed_path)\n",
    "    examples = load_fewshot_examples(train, test, preprocessed)\n",
    "    sentences = []\n",
    "    if (not checkpoint == \"\" and os.path.exists(checkpoint)):\n",
    "        ckpt = pd.read_csv(checkpoint)\n",
    "        sentences_pd = list(ckpt['fewshot output'])\n",
    "        sentences = []\n",
    "        for i, s in enumerate(sentences_pd):\n",
    "            if not type(s) == float:\n",
    "                sentences.append(s)\n",
    "            else:\n",
    "                try:\n",
    "                    sentences.append(generate_fewshot(examples.iloc[i], k))\n",
    "                except:\n",
    "                    # print(\"---\")\n",
    "                    # print(\"ERROR:  DUMPING GENERATED SENTENCES!\")\n",
    "                    # print()\n",
    "                    # print(sentences)\n",
    "                    # print()\n",
    "                    # print(\"ERROR ON \" + examples.iloc[i]['original'])\n",
    "                    # print(\"---\")\n",
    "                    sentences.append(\"\")\n",
    "            exit = True\n",
    "            for s in sentences_pd[i:]:\n",
    "                if not type(s) == float:\n",
    "                    exit = False\n",
    "            if exit:\n",
    "                break\n",
    "    for i in tqdm(range(len(examples))):\n",
    "        if i < len(sentences):\n",
    "            continue\n",
    "        row = examples.iloc[i]\n",
    "        try:\n",
    "            sentences.append(generate_fewshot(row, k))\n",
    "        except:\n",
    "            # print(\"---\")\n",
    "            # print(\"ERROR:  DUMPING GENERATED SENTENCES!\")\n",
    "            # print()\n",
    "            # print(sentences)\n",
    "            # print()\n",
    "            # print(\"ERROR ON \" + row['original'])\n",
    "            # print(\"---\")\n",
    "            sentences.append(\"\")\n",
    "    if not output_csv == \"\":\n",
    "        output = {\"original\":list(test['original']), \"fewshot output\": sentences}\n",
    "        output_df = pd.DataFrame(output)\n",
    "        output_df.to_csv(output_csv, index=False)\n",
    "    bleu, sari = calc_bleu_sari(test, sentences)\n",
    "    return bleu, sari\n",
    "\n",
    "# Try k-shots to fill in blanks, but if the error persists try k-=1\n",
    "def few_shot_backoff(train_path, test_path, preprocessed_path, k=3, output_csv=\"\", checkpoint=\"\"):\n",
    "    train = pd.read_csv(train_path)\n",
    "    test = pd.read_csv(test_path)\n",
    "    preprocessed = pd.read_csv(preprocessed_path)\n",
    "    examples = load_fewshot_examples(train, test, preprocessed)\n",
    "    sentences = []\n",
    "    if (not checkpoint == \"\" and os.path.exists(checkpoint)):\n",
    "        ckpt = pd.read_csv(checkpoint)\n",
    "        sentences_pd = list(ckpt['fewshot output'])\n",
    "        sentences = []\n",
    "        for i, s in tqdm(enumerate(sentences_pd)):\n",
    "            if not type(s) == float:\n",
    "                sentences.append(s)\n",
    "            else:\n",
    "                curr_k = k\n",
    "                while curr_k >= 0:\n",
    "                    try:\n",
    "                        generated = generate_fewshot(examples.iloc[i], curr_k)\n",
    "                        sentences.append(generated)\n",
    "                        break\n",
    "                    except:\n",
    "                        curr_k -= 1\n",
    "                if curr_k < 0:\n",
    "                    print(\"ERROR ON INPUT: \" + examples.iloc[i]['original'])\n",
    "        if not output_csv == \"\":\n",
    "            output = {\"original\":list(test['original']), \"fewshot output\": sentences}\n",
    "            output_df = pd.DataFrame(output)\n",
    "            output_df.to_csv(output_csv, index=False)\n",
    "        bleu, sari = calc_bleu_sari(test, sentences)\n",
    "        return bleu, sari\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 23/23 [00:00<00:00, 694021.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU 29.938036356557735\n",
      "SARI 34.4325805637174\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# bleu, sari = fewshot_eval(\"../data/English/ASSET_train.csv\", \"../data/English/ASSET_test.csv\", \"./few_shot_preprocessing/ASSET_test_random.csv\", k=3, output_csv=\"../../fewshot-outputs/ASSET/3.rand.csv\", checkpoint=\"../../fewshot-outputs/ASSET/3.rand.csv\")\n",
    "# bleu, sari = fewshot_eval(\"../data/Urdu/SimplifyUR_train.csv\", \"../data/Urdu/SimplifyUR_test.csv\", \"./few_shot_preprocessing/SimplifyUR_test_similarity.csv\", k=0, output_csv=\"../../fewshot-outputs/SimplifyUR/0.sim.csv\", checkpoint=\"../../fewshot-outputs/SimplifyUR/0.sim.csv\")\n",
    "bleu, sari = fewshot_eval(\"../data/Basque/CBST_train.csv\", \"../data/Basque/CBST_test.csv\", \"./few_shot_preprocessing/CBST_test_similarity.csv\", k=0, output_csv=\"../../fewshot-outputs/CBST/0.sim.csv\", checkpoint=\"../../fewshot-outputs/CBST/0.sim.csv\")\n",
    "print(\"BLEU\", bleu)\n",
    "print(\"SARI\", sari)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TESTING 0-SHOT:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 2853268.03it/s]\n",
      "100it [00:00, 1823610.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU 11.689742698596966\n",
      "SARI 30.529485143245168\n",
      "TESTING 1-SHOT:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 2267191.35it/s]\n",
      "100it [00:00, 1959955.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU 17.091509721695978\n",
      "SARI 46.08253360482048\n",
      "TESTING 2-SHOT:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 2219208.47it/s]\n",
      "100it [00:00, 1923992.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU 19.35726793526695\n",
      "SARI 49.39268449407829\n",
      "TESTING 3-SHOT:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 2621440.00it/s]\n",
      "100it [00:00, 1664406.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU 25.344205034474086\n",
      "SARI 47.00969273624047\n",
      "TESTING 5-SHOT:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 2219208.47it/s]\n",
      "100it [00:00, 2267191.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU 20.316525103528605\n",
      "SARI 49.30464091007859\n",
      "TESTING 10-SHOT:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 2046001.95it/s]\n",
      "100it [00:00, 2129088.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU 27.308727335578766\n",
      "SARI 47.503824026412374\n",
      "TESTING 20-SHOT:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 2452809.36it/s]\n",
      "100it [00:00, 2076388.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU 28.250291891552635\n",
      "SARI 46.55381480754754\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_set = \"../data/Japanese/Easy Japanese Extended_train.csv\"\n",
    "test_set = \"../data/Japanese/Easy Japanese Extended_test.csv\"\n",
    "name = \"EasyJAExt\"\n",
    "\n",
    "split = \"test\"\n",
    "demonstration = \"similarity\" # \"similarity\" or \"random\"\n",
    "\n",
    "k_shots = [0,1,2,3,5,10,20]\n",
    "\n",
    "for k in k_shots:\n",
    "    print(\"TESTING \" + str(k) + \"-SHOT:\")\n",
    "    mapping = \"./few_shot_preprocessing/\" + name + \"_\" + split + \"_\" + demonstration + \".csv\"\n",
    "    dem = \"sim\" if (demonstration == \"similarity\") else (\"rand\" if (demonstration == \"random\") else \"unk\")\n",
    "    output = \"../../fewshot-outputs/\" + name + \"/\" + str(k) + \".\" + dem + \".csv\"\n",
    "    bleu, sari = fewshot_eval(train_set, test_set, mapping, k=k, output_csv=output, checkpoint=output)\n",
    "    bleu, sari = few_shot_backoff(train_set, test_set, mapping, k=k, output_csv=output, checkpoint=output)\n",
    "\n",
    "    print(\"BLEU\", bleu)\n",
    "    print(\"SARI\", sari)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TESTING 1-SHOT:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 2076388.12it/s]\n",
      "100it [00:00, 2343186.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU 12.717909340095272\n",
      "SARI 40.70364494524691\n",
      "TESTING 2-SHOT:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 2796202.67it/s]\n",
      "100it [00:00, 1657827.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU 16.327023748425336\n",
      "SARI 41.381299000171616\n",
      "TESTING 3-SHOT:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 2330168.89it/s]\n",
      "100it [00:00, 2006844.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU 17.700870844368012\n",
      "SARI 40.037339935348065\n",
      "TESTING 5-SHOT:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 2086718.41it/s]\n",
      "100it [00:00, 2330168.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU 17.117299016458176\n",
      "SARI 42.55154266686833\n",
      "TESTING 10-SHOT:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 2511559.28it/s]\n",
      "100it [00:00, 2231012.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU 22.609779504090277\n",
      "SARI 42.49635550070905\n",
      "TESTING 20-SHOT:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 2330168.89it/s]\n",
      "100it [00:00, 2036069.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU 19.26750449984209\n",
      "SARI 42.110943041781155\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "demonstration = \"random\" # \"similarity\" or \"random\"\n",
    "\n",
    "k_shots = [1,2,3,5,10,20]\n",
    "\n",
    "for k in k_shots:\n",
    "    print(\"TESTING \" + str(k) + \"-SHOT:\")\n",
    "    mapping = \"./few_shot_preprocessing/\" + name + \"_\" + split + \"_\" + demonstration + \".csv\"\n",
    "    dem = \"sim\" if (demonstration == \"similarity\") else (\"rand\" if (demonstration == \"random\") else \"unk\")\n",
    "    output = \"../../fewshot-outputs/\" + name + \"/\" + str(k) + \".\" + dem + \".csv\"\n",
    "    bleu, sari = fewshot_eval(train_set, test_set, mapping, k=k, output_csv=output, checkpoint=output)\n",
    "    bleu, sari = few_shot_backoff(train_set, test_set, mapping, k=k, output_csv=output, checkpoint=output)\n",
    "\n",
    "    print(\"BLEU\", bleu)\n",
    "    print(\"SARI\", sari)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9it [00:10,  1.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR ON INPUT: DMI lover en lidt vejrmæssig kedelig weekend med gråvejr , byger og blæst .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10it [00:15,  1.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR ON INPUT: Men nu er folk pludselig begyndt at tænke på , at det i virkeligheden kan have en pris kun at fokusere på sig selv , siger generalsekretæren til avisen . \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11it [00:19,  2.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR ON INPUT: Den NUM . februar .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14it [00:23,  1.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR ON INPUT: Beslutningen er taget i fællesskab , lyder det .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15it [00:27,  2.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR ON INPUT: Vagn Jelsøe forklarer , at hvis fire piger køber fire par sko på én gang , fordi portoen så bliver billigere , skal de muligvis bevise , at skoene kun er til eget brug .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "17it [00:32,  2.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR ON INPUT: Kammerater og sanitetspersonel ydede førstehjælp på stedet .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "18it [00:36,  2.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR ON INPUT: Rækken af koncerter nåede aldrig at blive til noget , da Michael Jackson blev fundet død i sit hjem i Los Angeles den NUM . juni .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22it [00:40,  1.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR ON INPUT: Piloten blev dræbt , da et passagerfly tirsdag kørte af landingsbanen under et uvejr på den thailandske ferieø Koh Samui .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24it [00:44,  1.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR ON INPUT: En kendelse , der blev kæret på stedet .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30it [00:48,  1.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR ON INPUT: - Der bliver simpelthen brugt for mange penge i Nato på ting , der ikke er målrettet missioner .\n"
     ]
    }
   ],
   "source": [
    "few_shot_backoff(\"../data/Danish/DSim Corpus_train.csv\", \"../data/Danish/DSim Corpus_test.csv\", \"./few_shot_preprocessing/DSim_test_random.csv\", k=20, output_csv=\"../../fewshot-outputs/DSim/20.rand.csv\", checkpoint=\"../../fewshot-outputs/DSim/20.rand.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_dump(csv_path, test, sentences):\n",
    "    while (len(sentences) < len(test)):\n",
    "        sentences.append(\"\")\n",
    "    output = {\"original\":list(test['original']), \"fewshot output\": sentences}\n",
    "    output_df = pd.DataFrame(output)\n",
    "    output_df.to_csv(csv_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = []\n",
    "\n",
    "csv_path = \"../../fewshot-outputs/ASSET/10.sim.csv\"\n",
    "test = pd.read_csv(\"../data/English/ASSET_test.csv\")\n",
    "\n",
    "save_dump(csv_path, test, sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_average_distance(distances, k=3, offset=0):\n",
    "    return np.average(np.average(distances[:,offset:offset+k], axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def similarity_experiment(train_path, test_path, preprocessed_path, preprocessed_dists, k=3, offset=0, output_csv=\"\", checkpoint=\"\"):\n",
    "    train = pd.read_csv(train_path)\n",
    "    test = pd.read_csv(test_path)\n",
    "    preprocessed = pd.read_csv(preprocessed_path)\n",
    "    preprocessed_dists = pd.read_csv(preprocessed_dists)\n",
    "    examples = load_fewshot_examples(train, test, preprocessed, offset=offset)\n",
    "    sentences = []\n",
    "    if (not checkpoint == \"\" and os.path.exists(checkpoint)):\n",
    "        ckpt = pd.read_csv(checkpoint)\n",
    "        sentences_pd = list(ckpt['fewshot output'])\n",
    "        sentences = []\n",
    "        for i, s in enumerate(sentences_pd):\n",
    "            if not type(s) == float:\n",
    "                sentences.append(s)\n",
    "            else:\n",
    "                try:\n",
    "                    sentences.append(generate_fewshot(examples.iloc[i], k))\n",
    "                except:\n",
    "                    print(\"---\")\n",
    "                    print(\"ERROR:  DUMPING GENERATED SENTENCES!\")\n",
    "                    print()\n",
    "                    print(sentences)\n",
    "                    print()\n",
    "                    print(\"ERROR ON \" + examples.iloc[i]['original'])\n",
    "                    print(\"---\")\n",
    "                    sentences.append(\"\")\n",
    "            exit = True\n",
    "            for s in sentences_pd[i:]:\n",
    "                if not type(s) == float:\n",
    "                    exit = False\n",
    "            if exit:\n",
    "                break\n",
    "    for i in tqdm(range(len(examples))):\n",
    "        if i < len(sentences):\n",
    "            continue\n",
    "        row = examples.iloc[i]\n",
    "        try:\n",
    "            sentences.append(generate_fewshot(row, k))\n",
    "        except:\n",
    "            print(\"---\")\n",
    "            print(\"ERROR:  DUMPING GENERATED SENTENCES!\")\n",
    "            print()\n",
    "            print(sentences)\n",
    "            print()\n",
    "            print(\"ERROR ON \" + row['original'])\n",
    "            print(\"---\")\n",
    "            sentences.append(\"\")\n",
    "    if not output_csv == \"\":\n",
    "        output = {\"original\":list(test['original']), \"fewshot output\": sentences}\n",
    "        output_df = pd.DataFrame(output)\n",
    "        output_df.to_csv(output_csv, index=False)\n",
    "    bleu, sari = calc_bleu_sari(test, sentences)\n",
    "    dist = find_average_distance(preprocessed_dists.to_numpy()[1:,1:], k, offset)\n",
    "    return bleu, sari, dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = \"../data/Urdu/SimplifyUR_train.csv\"\n",
    "test_set = \"../data/Urdu/SimplifyUR_test.csv\"\n",
    "name = \"SimplifyUR\"\n",
    "\n",
    "offsets = [0, 95, 195, 295, 395, 494]\n",
    "k = 5\n",
    "\n",
    "for offset in offsets:\n",
    "    print(\"TESTING \" + str(k) + \"-SHOT OFFSET \" + str(offset) + \":\")\n",
    "    mapping = \"./few_shot_preprocessing/\"+name+\"_sim_experiment.csv\"\n",
    "    dist = \"./few_shot_preprocessing/\"+name+\"_sim_experiment_dist.csv\"\n",
    "    output = \"../../fewshot-sim-experiments/\" + name + \"/\" + str(offset) + \"-\" + str((offset+k)) + \".csv\"\n",
    "    bleu, sari, dist = similarity_experiment(train_set, test_set, mapping, dist, k=k, offset=offset, output_csv=output, checkpoint=output)\n",
    "\n",
    "    print(\"BLEU\", bleu)\n",
    "    print(\"SARI\", sari)\n",
    "    print(\"DIST\", dist)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "few_shot_backoff(\"../data/Urdu/SimplifyUR_train.csv\", \"../data/Urdu/SimplifyUR_test.csv\", \"./few_shot_preprocessing/SimplifyUR_test_similarity.csv\", k=5, output_csv=\"../../fewshot-outputs/SimplifyUR/5.sim.csv\", checkpoint=\"../../fewshot-outputs/SimplifyUR/5.sim.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8 (main, Oct 13 2022, 09:48:40) [Clang 14.0.0 (clang-1400.0.29.102)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
