{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from easse.sari import corpus_sari, get_corpus_sari_operation_scores\n",
    "from sacrebleu import corpus_bleu\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_bleu_sari(original, sentences, references, tokenizer='13a'):\n",
    "\n",
    "    num_refs = max([len(refs) for refs in references])\n",
    "\n",
    "    bleu_scores = np.zeros((num_refs))\n",
    "    sari_scores = np.zeros((num_refs))\n",
    "\n",
    "    examples = [{\"original\": [], \"sentences\": [], \"references\": []} for _ in range(num_refs)]\n",
    "\n",
    "    assert len(original) == len(sentences)\n",
    "    assert len(sentences) == len(references)\n",
    "\n",
    "    for original, refs, sentence in zip(original, references, sentences):\n",
    "        simple = sentence\n",
    "        num_ref = len(refs)\n",
    "        examples[num_ref-1]['original'].append(original)\n",
    "        examples[num_ref-1]['sentences'].append(simple)\n",
    "        examples[num_ref-1]['references'].append(refs)\n",
    "\n",
    "    counts = np.array([len(e['original']) for e in examples])\n",
    "    total = sum(counts)\n",
    "    weights = np.divide(counts, total)\n",
    "\n",
    "    for i in range(len(examples)):\n",
    "        if counts[i] > 0:\n",
    "            references = np.array(examples[i]['references']).T.tolist()\n",
    "            bleu_scores[i] = corpus_bleu(\n",
    "                                examples[i]['sentences'],\n",
    "                                references,\n",
    "                                force = True,\n",
    "                                tokenize = tokenizer,\n",
    "                                lowercase = True\n",
    "                            ).score\n",
    "            sari_scores[i] = corpus_sari(\n",
    "                                orig_sents = examples[i]['original'],\n",
    "                                sys_sents = examples[i]['sentences'],\n",
    "                                refs_sents = references,\n",
    "                                tokenizer=tokenizer\n",
    "                            )\n",
    "    \n",
    "    bleu = np.dot(bleu_scores, weights)\n",
    "    sari = np.dot(sari_scores, weights)\n",
    "\n",
    "    return bleu, sari"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_words(text):\n",
    "    return text.split()\n",
    "\n",
    "def truncate(sentence):\n",
    "    # Take first 80% words\n",
    "    words = to_words(sentence)\n",
    "    return ' '.join(words[: int(len(words) * 0.8)]) + '.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## MODIFY THIS\n",
    "dataset = \"TSSlovene\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset multilingual_simplification/TSSlovene to /Users/michaelryan/.cache/huggingface/datasets/multilingual_simplification/TSSlovene/1.0.0/1ff38e5f95caa94278642e100d418265055dbe25b26e4bff4eb2be51de59a924...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65ee33617a7d4f4481511757fbdf9220",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "647a630d930f4238b868d3b029533342",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be7413f2b9df4e339813cefb27219b09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset multilingual_simplification downloaded and prepared to /Users/michaelryan/.cache/huggingface/datasets/multilingual_simplification/TSSlovene/1.0.0/1ff38e5f95caa94278642e100d418265055dbe25b26e4bff4eb2be51de59a924. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3334f6b676b74c6eb9eb2a1c391dcb5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "raw_datasets = load_dataset(\"./MultilingualSimplification.py\", name=dataset)\n",
    "\n",
    "test_set = raw_datasets[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IDENTITY\n",
      "BLEU 7.761789172194769\n",
      "SARI 5.934345427797628\n",
      "\n",
      "TRUNCATE\n",
      "BLEU 6.085824291507021\n",
      "SARI 19.027115202574883\n"
     ]
    }
   ],
   "source": [
    "print(\"IDENTITY\")\n",
    "bleu, sari = calc_bleu_sari(test_set[\"original\"], test_set[\"original\"], [ex['simplifications'] for ex in test_set[\"simple\"]])\n",
    "print(\"BLEU\", bleu)\n",
    "print(\"SARI\", sari)\n",
    "\n",
    "print()\n",
    "\n",
    "print(\"TRUNCATE\")\n",
    "bleu, sari = calc_bleu_sari(test_set[\"original\"], [truncate(sentence) for sentence in test_set[\"original\"]], [ex['simplifications'] for ex in test_set[\"simple\"]])\n",
    "print(\"BLEU\", bleu)\n",
    "print(\"SARI\", sari)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset multilingual_simplification/EasyJapaneseExtended to /Users/michaelryan/.cache/huggingface/datasets/multilingual_simplification/EasyJapaneseExtended/1.0.0/1ff38e5f95caa94278642e100d418265055dbe25b26e4bff4eb2be51de59a924...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ed718b5fe12401089872d5b8c0a66fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1887d482a11845cfa4aef3959fe3b839",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "270c59f17ed3475ca2cccb6b3122ab34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset multilingual_simplification downloaded and prepared to /Users/michaelryan/.cache/huggingface/datasets/multilingual_simplification/EasyJapaneseExtended/1.0.0/1ff38e5f95caa94278642e100d418265055dbe25b26e4bff4eb2be51de59a924. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "295cba3e5d134e9d96931b7225d7a662",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IDENTITY\n",
      "BLEU 20.230626941116604\n",
      "SARI 9.000209549720857\n",
      "\n",
      "TRUNCATE\n",
      "BLEU 8.812058613684762\n",
      "SARI 43.8435071245527\n"
     ]
    }
   ],
   "source": [
    "dataset = \"EasyJapaneseExtended\"\n",
    "\n",
    "raw_datasets = load_dataset(\"./MultilingualSimplification.py\", name=dataset)\n",
    "\n",
    "test_set = raw_datasets[\"test\"]\n",
    "\n",
    "print(\"IDENTITY\")\n",
    "bleu, sari = calc_bleu_sari(test_set[\"original\"], test_set[\"original\"], [ex['simplifications'] for ex in test_set[\"simple\"]], 'intl')\n",
    "print(\"BLEU\", bleu)\n",
    "print(\"SARI\", sari)\n",
    "\n",
    "print()\n",
    "\n",
    "print(\"TRUNCATE\")\n",
    "bleu, sari = calc_bleu_sari(test_set[\"original\"], [sentence[: int(len(sentence) * 0.8)] + \"ã€‚\" for sentence in test_set[\"original\"]], [ex['simplifications'] for ex in test_set[\"simple\"]], 'intl')\n",
    "print(\"BLEU\", bleu)\n",
    "print(\"SARI\", sari)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = [\"NewselaEN\", \"WikiAutoEN\", \"ASSET\", \"Simplext\", \"NewselaES\", \"Terence\", \"Teacher\", \"SimpitikiWiki\",\n",
    "\"AdminIt\", \"PaCCSS-IT\", \"CLEAR\", \"WikiLargeFR\", \"EasyJapanese\", \"EasyJapaneseExtended\", \"PorSimples\", \"TextComplexityDE\", \n",
    "\"GEOLinoTest\", \"GermanNews\", \"CBST\", \"DSim\", \"SimplifyUR\", \"RuWikiLarge\", \"RSSE\", \"RuAdaptLit\", \"RuAdaptFairytales\", \"RuAdaptEncy\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_bleu_sari_parts(df_ref, sentences, tokenizer='13a'):\n",
    "\n",
    "    num_refs = df_ref.shape[1]-1\n",
    "\n",
    "    bleu_scores = np.zeros((num_refs))\n",
    "    sari_keep_scores = np.zeros((num_refs))\n",
    "    sari_add_scores = np.zeros((num_refs))\n",
    "    sari_del_scores = np.zeros((num_refs))\n",
    "\n",
    "    sentences = [sent if type(sent) == str else \"\" for sent in sentences]\n",
    "\n",
    "    examples = [{\"original\": [], \"sentences\": [], \"references\": []} for _ in range(num_refs)]\n",
    "\n",
    "    assert df_ref.shape[0] == len(sentences)\n",
    "\n",
    "    for (index,row), sentence in zip(df_ref.iterrows(), sentences):\n",
    "        original = row['original']\n",
    "        simple = sentence\n",
    "        ref_list = []\n",
    "        for col in row.index:\n",
    "            if col != 'original' and type(row[col]) != float:\n",
    "                ref_list.append(row[col])\n",
    "        num_ref = len(ref_list)\n",
    "        examples[num_ref-1]['original'].append(original)\n",
    "        examples[num_ref-1]['sentences'].append(simple)\n",
    "        examples[num_ref-1]['references'].append(ref_list)\n",
    "\n",
    "    counts = np.array([len(e['original']) for e in examples])\n",
    "    total = sum(counts)\n",
    "    weights = np.divide(counts, total)\n",
    "\n",
    "    for i in range(len(examples)):\n",
    "        if counts[i] > 0:\n",
    "            references = np.array(examples[i]['references']).T.tolist()\n",
    "            bleu_scores[i] = corpus_bleu(\n",
    "                                examples[i]['sentences'],\n",
    "                                references,\n",
    "                                force = True,\n",
    "                                tokenize = tokenizer,\n",
    "                                lowercase = True\n",
    "                            ).score\n",
    "            sari_add_scores[i], sari_keep_scores[i], sari_del_scores[i] = get_corpus_sari_operation_scores(\n",
    "                                orig_sents = examples[i]['original'],\n",
    "                                sys_sents = examples[i]['sentences'],\n",
    "                                refs_sents = references,\n",
    "                                tokenizer = tokenizer\n",
    "                            )\n",
    "\n",
    "    bleu = np.dot(bleu_scores, weights)\n",
    "    sari_add = np.dot(sari_add_scores, weights)\n",
    "    sari_keep = np.dot(sari_keep_scores, weights)\n",
    "    sari_del = np.dot(sari_del_scores, weights)\n",
    "\n",
    "    return bleu, sari_add, sari_keep, sari_del"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "subcorpora = {\n",
    "    \"NewselaEN\": {\n",
    "        \"path\": \"./data/English/Newsela EN\",\n",
    "        \"language\": \"en\"\n",
    "    },\n",
    "    \"WikiAutoEN\": {\n",
    "        \"path\": \"./data/English/WikiAuto\",\n",
    "        \"language\": \"en\"\n",
    "    },\n",
    "    \"ASSET\": {\n",
    "        \"path\": \"./data/English/ASSET\",\n",
    "        \"language\": \"en\"\n",
    "    },\n",
    "    \"Simplext\": {\n",
    "        \"path\": \"./data/Spanish/Simplext\",\n",
    "        \"language\": \"es\"\n",
    "    },\n",
    "    \"NewselaES\": {\n",
    "        \"path\": \"./data/Spanish/Newsela ES\",\n",
    "        \"language\": \"es\"\n",
    "    },\n",
    "    \"Terence\": {\n",
    "        \"path\" : \"./data/Italian/Terence\",\n",
    "        \"language\": \"it\"\n",
    "    },\n",
    "    \"Teacher\": {\n",
    "        \"path\": \"./data/Italian/Teacher\",\n",
    "        \"language\": \"it\"\n",
    "    },\n",
    "    \"SimpitikiWiki\": {\n",
    "        \"path\": \"./data/Italian/Simpitiki Italian Wikipedia\",\n",
    "        \"language\": \"it\"\n",
    "    },\n",
    "    \"AdminIt\": {\n",
    "        \"path\": \"./data/Italian/AdminIT\",\n",
    "        \"language\": \"it\"\n",
    "    },\n",
    "    \"PaCCSS-IT\": {\n",
    "        \"path\": \"./data/Italian/PaCCSS-IT Corpus\",\n",
    "        \"language\": \"it\"\n",
    "    },\n",
    "    \"CLEAR\" : {\n",
    "        \"path\" : \"./data/French/CLEAR Corpus\",\n",
    "        \"language\": \"fr\"\n",
    "    },\n",
    "    \"WikiLargeFR\": {\n",
    "        \"path\" : \"./data/French/WikiLargeFR Corpus\",\n",
    "        \"language\": \"fr\"\n",
    "    },\n",
    "    \"EasyJapanese\": {\n",
    "        \"path\": \"./data/Japanese/Easy Japanese Corpus\",\n",
    "        \"language\": \"ja\"\n",
    "    },\n",
    "    \"EasyJapaneseExtended\": {\n",
    "        \"path\": \"./data/Japanese/Easy Japanese Extended\",\n",
    "        \"language\": \"ja\"\n",
    "    },\n",
    "    \"PorSimples\" : {\n",
    "        \"path\": \"./data/Brazilian Portuguese/PorSimples\",\n",
    "        \"language\": \"pt-br\"\n",
    "    },\n",
    "    \"TextComplexityDE\" : {\n",
    "        \"path\": \"./data/German/TextComplexityDE Parallel Corpus\",\n",
    "        \"language\": \"de\"\n",
    "    },\n",
    "    \"GEOLinoTest\" : {\n",
    "        \"path\" : \"./data/German/GEOLino Corpus\",\n",
    "        \"language\": \"de\"\n",
    "    },\n",
    "    \"GermanNews\" : {\n",
    "        \"path\" : \"./data/German/German News\",\n",
    "        \"language\": \"de\"\n",
    "    },\n",
    "    \"CBST\": {\n",
    "        \"path\" : \"./data/Basque/CBST\",\n",
    "        \"language\": \"eu\"\n",
    "    },\n",
    "    \"DSim\": {\n",
    "        \"path\": \"./data/Danish/DSim Corpus\",\n",
    "        \"language\": \"da\"\n",
    "    },\n",
    "    \"SimplifyUR\": {\n",
    "        \"path\": \"./data/Urdu/SimplifyUR\",\n",
    "        \"language\": \"ur\"\n",
    "    },\n",
    "    \"RuWikiLarge\": {\n",
    "        \"path\" : \"./data/Russian/RuWikiLarge\",\n",
    "        \"language\": \"ru\"\n",
    "    },\n",
    "    \"RSSE\" : {\n",
    "        \"path\": \"./data/Russian/RSSE Corpus\",\n",
    "        \"language\": \"ru\"\n",
    "    },\n",
    "    \"RuAdaptLit\" : {\n",
    "        \"path\": \"./data/Russian/RuAdapt Literature\",\n",
    "        \"language\": \"ru\"\n",
    "    },\n",
    "    \"RuAdaptFairytales\" : {\n",
    "        \"path\": \"./data/Russian/RuAdapt Fairytales\",\n",
    "        \"language\": \"ru\"\n",
    "    },\n",
    "    \"RuAdaptEncy\" : {\n",
    "        \"path\" : \"./data/Russian/RuAdapt Ency\",\n",
    "        \"language\": \"ru\"\n",
    "    },\n",
    "    \"TSSlovene\" : {\n",
    "        \"path\" : \"./data/Slovene/Text Simplification Slovene\",\n",
    "        \"language\": \"sl\"\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NewselaEN\n",
      "IDENTITY\n",
      "NewselaEN,0.0,78.511591740056,0.0\n",
      "\n",
      "NewselaEN\n",
      "TRUNCATE\n",
      "NewselaEN,0.3890134752767037,68.63954316734663,29.658434033531325\n",
      "\n",
      "WikiAutoEN\n",
      "IDENTITY\n",
      "WikiAutoEN,0.0,62.78409703329144,0.0\n",
      "\n",
      "WikiAutoEN\n",
      "TRUNCATE\n",
      "WikiAutoEN,0.17481053218016177,57.866935305343645,36.32123901972068\n",
      "\n",
      "ASSET\n",
      "IDENTITY\n",
      "ASSET,0.0,62.201479040615006,0.0\n",
      "\n",
      "ASSET\n",
      "TRUNCATE\n",
      "ASSET,0.21134294555943267,55.04221791958657,33.735078737222885\n",
      "\n",
      "Simplext\n",
      "IDENTITY\n",
      "Simplext,0.0,23.83033499752721,0.0\n",
      "\n",
      "Simplext\n",
      "TRUNCATE\n",
      "Simplext,0.5757725280948548,23.986912417790066,36.261247733672576\n",
      "\n",
      "NewselaES\n",
      "IDENTITY\n",
      "NewselaES,0.0,72.63455558970311,0.0\n",
      "\n",
      "NewselaES\n",
      "TRUNCATE\n",
      "NewselaES,0.12594589496927475,64.07410927209675,30.716482047510794\n",
      "\n",
      "Terence\n",
      "IDENTITY\n",
      "Terence,0.0,80.50374388524759,0.0\n",
      "\n",
      "Terence\n",
      "TRUNCATE\n",
      "Terence,0.40816326530612246,69.31341402250749,28.741627391748743\n",
      "\n",
      "Teacher\n",
      "IDENTITY\n",
      "Teacher,0.0,52.22341206564713,0.0\n",
      "\n",
      "Teacher\n",
      "TRUNCATE\n",
      "Teacher,0.48543689320388345,47.46083296920002,35.29770927364417\n",
      "\n",
      "SimpitikiWiki\n",
      "IDENTITY\n",
      "SimpitikiWiki,0.0,97.35577729707038,0.0\n",
      "\n",
      "SimpitikiWiki\n",
      "TRUNCATE\n",
      "SimpitikiWiki,0.0,86.3058583997991,9.70895619370685\n",
      "\n",
      "AdminIt\n",
      "IDENTITY\n",
      "AdminIt,0.0,62.67099736940258,0.0\n",
      "\n",
      "AdminIt\n",
      "TRUNCATE\n",
      "AdminIt,0.0,56.43043051734573,28.225530487452218\n",
      "\n",
      "PaCCSS-IT\n",
      "IDENTITY\n",
      "PaCCSS-IT,0.0,54.42925170450363,0.0\n",
      "\n",
      "PaCCSS-IT\n",
      "TRUNCATE\n",
      "PaCCSS-IT,1.5390295104669864,46.970829552941424,36.28054197565199\n",
      "\n",
      "CLEAR\n",
      "IDENTITY\n",
      "CLEAR,0.0,71.20165307500888,0.0\n",
      "\n",
      "CLEAR\n",
      "TRUNCATE\n",
      "CLEAR,0.1354936554497821,64.91475212882114,31.450366388528515\n",
      "\n",
      "WikiLargeFR\n",
      "IDENTITY\n",
      "WikiLargeFR,0.0,73.32568268827524,0.0\n",
      "\n",
      "WikiLargeFR\n",
      "TRUNCATE\n",
      "WikiLargeFR,0.16268015904153268,65.43903996518809,31.081944814058957\n",
      "\n",
      "EasyJapanese\n",
      "IDENTITY\n",
      "EasyJapanese,0.0,73.91667764103883,0.0\n",
      "\n",
      "EasyJapanese\n",
      "TRUNCATE\n",
      "EasyJapanese,0.0,0.0,57.04301467192077\n",
      "\n",
      "EasyJapaneseExtended\n",
      "IDENTITY\n",
      "EasyJapaneseExtended,0.0,27.000628649162568,0.0\n",
      "\n",
      "EasyJapaneseExtended\n",
      "TRUNCATE\n",
      "EasyJapaneseExtended,0.0,0.0,88.31531783530491\n",
      "\n",
      "PorSimples\n",
      "IDENTITY\n",
      "PorSimples,0.0,84.63330477070704,0.0\n",
      "\n",
      "PorSimples\n",
      "TRUNCATE\n",
      "PorSimples,0.6943083071492359,71.01642646734634,22.038669847294877\n",
      "\n",
      "TextComplexityDE\n",
      "IDENTITY\n",
      "TextComplexityDE,0.0,46.25040979331227,0.0\n",
      "\n",
      "TextComplexityDE\n",
      "TRUNCATE\n",
      "TextComplexityDE,0.0,43.44016106458021,36.987312586712605\n",
      "\n",
      "GEOLinoTest\n",
      "IDENTITY\n",
      "GEOLinoTest,0.0,82.3428989965789,0.0\n",
      "\n",
      "GEOLinoTest\n",
      "TRUNCATE\n",
      "GEOLinoTest,0.0,68.8670315150439,23.245482700310905\n",
      "\n",
      "GermanNews\n",
      "IDENTITY\n",
      "GermanNews,0.0,16.815069200449486,0.0\n",
      "\n",
      "GermanNews\n",
      "TRUNCATE\n",
      "GermanNews,0.1609563947842003,17.106013787652362,35.79682081568714\n",
      "\n",
      "CBST\n",
      "IDENTITY\n",
      "CBST,0.0,70.36672687665657,0.0\n",
      "\n",
      "CBST\n",
      "TRUNCATE\n",
      "CBST,0.0,63.58805195997157,34.13829562544887\n",
      "\n",
      "DSim\n",
      "IDENTITY\n",
      "DSim,0.0,48.73905555313365,0.0\n",
      "\n",
      "DSim\n",
      "TRUNCATE\n",
      "DSim,0.2034573440579533,44.88134092309872,33.21387079439419\n",
      "\n",
      "SimplifyUR\n",
      "IDENTITY\n",
      "SimplifyUR,0.0,74.50583050084637,0.0\n",
      "\n",
      "SimplifyUR\n",
      "TRUNCATE\n",
      "SimplifyUR,0.0,63.08064576544627,30.81210694385583\n",
      "\n",
      "RuWikiLarge\n",
      "IDENTITY\n",
      "RuWikiLarge,0.0,72.72233817249008,0.0\n",
      "\n",
      "RuWikiLarge\n",
      "TRUNCATE\n",
      "RuWikiLarge,0.10391944553121617,64.1064480803716,31.411554617317307\n",
      "\n",
      "RSSE\n",
      "IDENTITY\n",
      "RSSE,0.0,32.72348507847457,0.0\n",
      "\n",
      "RSSE\n",
      "TRUNCATE\n",
      "RSSE,0.11213828817035729,29.990562859626433,38.045985335858774\n",
      "\n",
      "RuAdaptLit\n",
      "IDENTITY\n",
      "RuAdaptLit,0.0,67.97015478784226,0.0\n",
      "\n",
      "RuAdaptLit\n",
      "TRUNCATE\n",
      "RuAdaptLit,1.5930906539990548,59.556672268607976,34.66556407380205\n",
      "\n",
      "RuAdaptFairytales\n",
      "IDENTITY\n",
      "RuAdaptFairytales,0.0,31.904990322013727,0.0\n",
      "\n",
      "RuAdaptFairytales\n",
      "TRUNCATE\n",
      "RuAdaptFairytales,1.5438300548128712,28.874520512493262,44.10837377073674\n",
      "\n",
      "RuAdaptEncy\n",
      "IDENTITY\n",
      "RuAdaptEncy,0.0,89.68824669031555,0.0\n",
      "\n",
      "RuAdaptEncy\n",
      "TRUNCATE\n",
      "RuAdaptEncy,0.2365524094176617,73.97135803877448,19.07227183847728\n",
      "\n",
      "TSSlovene\n",
      "IDENTITY\n",
      "TSSlovene,0.0,17.803036283392885,0.0\n",
      "\n",
      "TSSlovene\n",
      "TRUNCATE\n",
      "TSSlovene,1.3369196734096227,16.01161389926124,39.73281203505379\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for dataset in subcorpora:\n",
    "    ref_df = pd.read_csv(subcorpora[dataset][\"path\"] + \"_test.csv\")\n",
    "\n",
    "    print(dataset)\n",
    "    print(\"IDENTITY\")\n",
    "    scores = calc_bleu_sari_parts(ref_df, ref_df['original'].to_list(), tokenizer='intl' if \"Japanese\" in dataset else '13a')\n",
    "    print(dataset + \",\" + str(scores[1]) + \",\" + str(scores[2]) + \",\" + str(scores[3]))\n",
    "\n",
    "    print()\n",
    "\n",
    "    print(dataset)\n",
    "    print(\"TRUNCATE\")\n",
    "    scores = calc_bleu_sari_parts(ref_df, [truncate(sentence) for sentence in ref_df['original'].to_list()], tokenizer='intl' if \"Japanese\" in dataset else '13a')\n",
    "    print(dataset + \",\" + str(scores[1]) + \",\" + str(scores[2]) + \",\" + str(scores[3]))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EasyJapanese\n",
      "TRUNCATE\n",
      "EasyJapanese,0.0,31.123073415945424,64.40100348204848\n",
      "\n",
      "EasyJapaneseExtended\n",
      "TRUNCATE\n",
      "EasyJapaneseExtended,0.0,35.93346590149011,95.597055472168\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for dataset in [\"EasyJapanese\", \"EasyJapaneseExtended\"]:\n",
    "    ref_df = pd.read_csv(subcorpora[dataset][\"path\"] + \"_test.csv\")\n",
    "    print(dataset)\n",
    "    print(\"TRUNCATE\")\n",
    "    scores = calc_bleu_sari_parts(ref_df, [sentence[: int(len(sentence) * 0.8)] + \"ã€‚\" for sentence in ref_df['original'].to_list()], tokenizer='intl' if \"Japanese\" in dataset else '13a')\n",
    "    print(dataset + \",\" + str(scores[1]) + \",\" + str(scores[2]) + \",\" + str(scores[3]))\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlts",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ca18bd5681216905704c89b7035c68c533f197e11f52b5f3035c7182103b2886"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
